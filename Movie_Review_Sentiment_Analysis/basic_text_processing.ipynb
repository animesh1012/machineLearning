{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "febd34cd",
   "metadata": {},
   "source": [
    "# Text Preprocessing for Sentiment Analysis\n",
    "\n",
    "Text preprocessing is a crucial step in sentiment analysis because raw text often contains noise that can confuse machine learning or deep learning models. Below are the key preprocessing steps with explanations and examples:\n",
    "\n",
    "---\n",
    "\n",
    "### Original Sentence:\n",
    "\"The movie was absolutely wonderful, and I loved every moment of it!\"\n",
    "\n",
    "### After Preprocessing:\n",
    "\n",
    "**Lowercased**:\n",
    "\n",
    "`the movie was absolutely wonderful, and i loved every moment of it!`\n",
    "\n",
    "**Remove punctuation**:\n",
    "\n",
    "`the movie was absolutely wonderful and i loved every moment of it`\n",
    "\n",
    "**Remove stopwords**:\n",
    "\n",
    "`movie absolutely wonderful loved every moment`\n",
    "\n",
    "**Tokenize**:\n",
    "\n",
    "`[\"movie\", \"absolutely\", \"wonderful\", \"loved\", \"every\", \"moment\"]`\n",
    "\n",
    "**Lemmatize**:\n",
    "\n",
    "`[\"movie\", \"absolutely\", \"wonderful\", \"love\", \"every\", \"moment\"]`\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Lowercasing  \n",
    "**Purpose:** Normalize the text so that \"Happy\" and \"happy\" are treated the same.  \n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "Text = \"I am HAPPY!\"\n",
    "Lowercased = \"i am happy!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e7930cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am happy!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am HAPPY!\"\n",
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea49df",
   "metadata": {},
   "source": [
    "## 2. Removing Punctuation\n",
    "Purpose: Punctuation usually does not carry sentiment and can be removed to simplify the text.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "Text = \"I am happy!!! Are you?\"\n",
    "Without_punctuation = \"I am happy Are you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f7ca6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am happy Are you'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuation = string.punctuation\n",
    "text = \"I am happy!!! Are you?\"\n",
    "without_punctuation = \"\".join(char for char in text if char not in punctuation)\n",
    "without_punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f841e",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "Purpose: Split the sentence into individual words (tokens).\n",
    "\n",
    "Example:\n",
    "```python\n",
    "Text = \"I love this product\"\n",
    "Tokens = [\"I\", \"love\", \"this\", \"product\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "933547d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'this', 'product']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text = \"I love this product\"\n",
    "Text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251c659e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'this', 'product']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "Text = \"I love this product\"\n",
    "tokens = word_tokenize(Text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e857e8",
   "metadata": {},
   "source": [
    "### Why nltk tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54010abf",
   "metadata": {},
   "source": [
    "- Using `.split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49e3d679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World!']\n"
     ]
    }
   ],
   "source": [
    "Text = \"Hello World!\"\n",
    "print(Text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadfefd6",
   "metadata": {},
   "source": [
    "- Using  `nltk.word_tokenize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "553c75e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World', '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "Text = \"Hello World!\"\n",
    "word_tokenize(Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35018112",
   "metadata": {},
   "source": [
    "## 4. Removing Stopwords\n",
    "Purpose: Stopwords (like is, am, the, in, and) are frequent words that don’t add much meaning for tasks like sentiment classification. Removing them helps reduce noise.\n",
    "\n",
    "Example (using NLTK stopwords):\n",
    "```python\n",
    "Text = \"I am very happy with the service\"\n",
    "After_removing_stopwords = \"happy service\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b5b1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy service'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "Text = \"I am very happy with the service\"\n",
    "tokens = word_tokenize(Text.lower())\n",
    "After_removing_stopwords = \" \".join([char for char in tokens if char not in stopwords])\n",
    "After_removing_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52587191",
   "metadata": {},
   "source": [
    "## 5. Stemming\n",
    "- **Goal**: Cut off prefixes or suffixes to get the root form of a word (*stem*).\n",
    "- **How it works**: Uses simple rules to chop off word endings. It doesn’t check if the result is a valid word.\n",
    "- **Examples**:\n",
    "  - `\"playing\"` → `\"play\"`\n",
    "  - `\"played\"` → `\"play\"`\n",
    "  - `\"flies\"` → `\"fli\"` ❌ *(not a real word)*\n",
    "- **Think of it like**: A quick-and-dirty way of shortening words.\n",
    "- **Use case**: When speed matters more than accuracy (e.g., search engines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ca40ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming\n",
      "Playing -> play\n",
      "Played -> play\n",
      "flies -> fli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "st = PorterStemmer()\n",
    "\n",
    "print(\"Stemming\")\n",
    "print(\"Playing ->\", st.stem(\"Playing\"))\n",
    "print(\"Played ->\", st.stem(\"Played\"))\n",
    "print(\"flies ->\", st.stem(\"flies\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136b2e0",
   "metadata": {},
   "source": [
    "## 6. Lemmatization\n",
    "- **Goal**: Convert a word to its **dictionary form** (*lemma*), considering its **meaning** and **part of speech**.\n",
    "- **How it works**: Uses vocabulary and grammar rules to return real words.\n",
    "- **Examples**:\n",
    "  - `\"playing\"` → `\"play\"`\n",
    "  - `\"better\"` → `\"good\"`\n",
    "  - `\"flies\"` → `\"fly\"` ✅ *(real word)*\n",
    "- **Think of it like**: A smarter, more accurate version of stemming.\n",
    "- **Use case**: When understanding and accuracy matter (e.g., chatbots, NLP pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3341c578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization\n",
      "Playing -> Playing\n",
      "Played -> Played\n",
      "flies -> fly\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "print(\"Lemmatization\")\n",
    "print(\"Playing ->\", lm.lemmatize(\"Playing\"))\n",
    "print(\"Played ->\", lm.lemmatize(\"Played\"))\n",
    "print(\"flies ->\", lm.lemmatize(\"flies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e51b93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
